{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgQ6KbOPiFh1"
      },
      "source": [
        "# Exercise 3: Statistical Learning Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoENfkbmiFh4"
      },
      "source": [
        "**Note**: Please insert the names of all participating students:\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMLVlGZCiFh5"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-luebeck/cs5450.git'\n",
        "    os.chdir('cs5450')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15cd0xN3iFh6"
      },
      "source": [
        "\"## Exercise 3.1: k-nearest-neighbor decision boundary\n",
        "\n",
        "\n",
        "In this exercise, we will use the [k-nearest-neighbor algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) for a two-class classification problem on a two-dimensional dataset. The data will be generated with the provided function gen_data. Each row in the generated matrix data corresponds to one observation and each column to a single feature. The generated array labels contains the corresponding desired output value of the two-class classification problem.\n",
        "\n",
        "When done correctly, your code should do the following:\n",
        "1. Create a random dataset using the function gen_data.\n",
        "2. Shuffle the data and its corresponding labels.\n",
        "3. Split the data into a disjoint train and test set.\n",
        "4. Train multiple k-nearest-neighbor classifiers with different k.\n",
        "5. Compute and print the train and test accuracy for each k.\n",
        "6. Visualize the data and the decision boundary of the best classifier with the function show_data.\n",
        "\n",
        "In case you are struggeling with the task, here are some helpful tips and hints:\n",
        "1. Useful functions : gen_data, ['permutation'](https://docs.scipy.org/doc/numpy-1.16.1/reference/generated/numpy.random.permutation.html), ['KNeighborsClassifier'](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier), show_data\n",
        "2. Good initial values are: n_clean=1000, n_noise=500, train_ratio=0.8, n_neighbors=[1,3,11,151,401].\n",
        "\n",
        "Bonus: Instead of a fixed train-test-split, implement a k-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIimi8gRiFh6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib notebook\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def gen_data(n_samples, n_noise):\n",
        "    \"\"\"\n",
        "    Generate noisy data.\n",
        "\n",
        "    INPUT:\n",
        "        n_samples : number of clean samples to generate\n",
        "        n_noise : number of noisy samples to generate\n",
        "\n",
        "    OUTPUT:\n",
        "       data : 2D data points\n",
        "       labels : ndarray of class labels with integer values 0 or 1\n",
        "    \"\"\"\n",
        "\n",
        "    #create clean data\n",
        "    clean_data = np.random.random((n_samples,2))\n",
        "    tmp = np.linalg.norm(clean_data, axis=1)\n",
        "    clean_labels = (tmp > 0.8).astype(int)\n",
        "\n",
        "    #create noisy data\n",
        "    noisy_data = np.random.random((n_noise,2))\n",
        "    noisy_labels = np.random.randint(0, 2, n_noise)\n",
        "\n",
        "    #merge data\n",
        "    data = np.concatenate((clean_data, noisy_data), 0)\n",
        "    labels = np.concatenate((clean_labels, noisy_labels), 0)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "def show_data(data, labels, classifier):\n",
        "    \"\"\"\n",
        "    Show the data and the decision boundary of the classifier.\n",
        "\n",
        "    INPUT:\n",
        "        data : 2D data points\n",
        "       labels : ndarray of class labels with integer values 0 or 1\n",
        "       classifier : trained classification model\n",
        "\n",
        "    OUTPUT:\n",
        "       fig : plt figure instance\n",
        "    \"\"\"\n",
        "    # create grid\n",
        "    nx = 10\n",
        "    ny = 10\n",
        "    x = np.linspace(0, 1, nx)\n",
        "    y = np.linspace(0, 1, ny)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    xy = np.stack((xx.reshape(-1),yy.reshape(-1)), 1)\n",
        "    # classify grid points\n",
        "    predictions = classifier.predict(xy)\n",
        "    predictions = predictions.reshape(ny, nx)\n",
        "\n",
        "    # plot classification boundary\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    im = ax.imshow(predictions, extent=(0,1,0,1), interpolation='bilinear', cmap='jet', origin='lower')\n",
        "    fig.colorbar(im)\n",
        "    # plot data points\n",
        "    ax.scatter(data[labels==0,0], data[labels==0,1], c=np.array([[0.5, 0.7, 1.]]), marker='o')\n",
        "    ax.scatter(data[labels==1,0], data[labels==1,1], c=np.array([[1., 0.6, 0.6]]), marker='x')\n",
        "\n",
        "    # write axis labels\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "G7j1c49biFh6"
      },
      "outputs": [],
      "source": [
        "#set seed for reproducable results\n",
        "np.random.seed(42)\n",
        "\n",
        "#TODO: choose number of clean samples\n",
        "n_clean = [1000]\n",
        "\n",
        "#TODO: choose number of noisy samples\n",
        "n_noise = [500]\n",
        "\n",
        "#TODO: choose ratio of the training samples\n",
        "train_ratio = [0.8]\n",
        "\n",
        "#TODO: choose which k to use for nearst-neighbor (create a list with many k)\n",
        "n_neighbors=[1,3,11,151,401]\n",
        "\n",
        "\"\"\"\n",
        "Train and visualize the classifier and its decision boundary\n",
        "\"\"\"\n",
        "\n",
        "#TODO: generate data (gen_data)\n",
        "data, labels = []\n",
        "\n",
        "#TODO: shuffle the data (np.random.permutation)\n",
        "\n",
        "#TODO: split the data into train and test set\n",
        "train_data = []\n",
        "train_labels = []\n",
        "test_data = []\n",
        "test_labels = []\n",
        "\n",
        "# loop over n_neighbors\n",
        "best_test_accuracy = 0.\n",
        "for k in n_neighbors:\n",
        "    #TODO: train knn-classifier (KNeighborsClassifier)\n",
        "\n",
        "    #TODO: compute empirical accuracy\n",
        "    train_accuracy = []\n",
        "\n",
        "    #TODO: compute test accuracy\n",
        "    test_accuracy = []\n",
        "\n",
        "    # print accuracies\n",
        "    print('k = {:3d} \\t train_accuracy = {:0.3f} \\t test_accuracy = {:0.3f}'.format(k, train_accuracy, test_accuracy))\n",
        "\n",
        "    #TODO: save the best classifier\n",
        "    best_classifier = []\n",
        "\n",
        "#TODO: visualize the data and the decision boundary of the best classifier (show_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC3r_eeIiFh7"
      },
      "source": [
        "## Exercise 3.2: Comprehension Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDrYyvUliFh7"
      },
      "source": [
        "Answer the following comprehension questions either with right or wrong and briefly explain your decision:\n",
        "\n",
        "1. In supervised learning, unlike unsupervised learning, there is a desired output value (label) associated with every sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0gp4nWOiFh7"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxDXIWjqiFh7"
      },
      "source": [
        "2. In classification, unlike regression, the output value takes continuous values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAKza7-2iFh7"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApDm_DiGiFh8"
      },
      "source": [
        "3. A 1-nearest-neighbor classifier has a low bias (structural risk) and a high variance (sample error)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajOEtXE1iFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG6K9LKxiFh8"
      },
      "source": [
        "4. The Vapnik Chervonenkis (VC) dimension is a measure of the complexity of a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjZWvYF_iFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3cAcT9tiFh8"
      },
      "source": [
        "5. The higher the VC dimension, the better the generalization capability of a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdcQAtUYiFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl63cje8iFh8"
      },
      "source": [
        "6. Given 1001 datapoints, a 1001-nearest-neighbor classifier has a high bias (structural risk) and a high variance (sample error)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsO3hjl5iFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTyJPpjziFh8"
      },
      "source": [
        "7. If the VC dimension is known, Hoeffding's inequality can be used to provide an upper bound on the test error of a classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaB_pMECiFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8tQFBlriFh8"
      },
      "source": [
        "8. Occam's razor states that one should prefer the simplest classifier that explains the data well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6ydswEQiFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDvwQ-4TiFh8"
      },
      "source": [
        "9. Empirical risk minimization minimizes the average error on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIfzKVSOiFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG9aXH2ziFh8"
      },
      "source": [
        "10. Structural risk minimization intends to balance the empirical risk and the confidence interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FcdgA29iFh8"
      },
      "source": [
        "Your answer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXNvenJBiFh9"
      },
      "source": [
        "11. In k-fold cross-validation, the original data is randomly partitioned into k equally sized overlapping subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx9x40eNiFh9"
      },
      "source": [
        "Your answer..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}